import torch
import gc
from threading import Thread
from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer
from peft import PeftModel
# ================= âš™ï¸ é…ç½®åŒºåŸŸ =================
# 1. åŸå§‹åº•åº§æ¨¡å‹è·¯å¾„ (1.5B)
BASE_MODEL_PATH = "./Models/Qwen/Qwen2.5-Coder-1.5B-Instruct"

# 2. ä½ çš„å…¨é‡å¾®è°ƒ Checkpoint è·¯å¾„
# æ³¨æ„ï¼šå…¨é‡å¾®è°ƒçš„ checkpoint åŒ…å«å®Œæ•´æƒé‡ï¼Œç›´æ¥åŠ è½½å³å¯
CHECKPOINT_PATH_FULL = "./output/qwen2.5_full_sft/checkpoint-2000"
CHECKPOINT_PATH_LORA = "./sft_cot_model/checkpoint-epoch-0-step-3200"
# 3. æµ‹è¯•é—®é¢˜
TEST_PROMPT = "create a java program that takes in 3 integers from the user and outputs the maximum number among them."


# ================= ğŸ› ï¸ æµå¼æ¨ç†å‡½æ•° =================
def stream_inference(model, tokenizer, prompt, title):
    print(f"\n{'=' * 20} {title} {'=' * 20}")

    # 1. æ„å»ºè¾“å…¥
    messages = [{"role": "user", "content": prompt}]
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer([text], return_tensors="pt").to(model.device)

    # 2. è®¾ç½®æµå¼è¾“å‡ºå™¨
    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

    # 3. é…ç½®ç”Ÿæˆå‚æ•°
    generation_kwargs = dict(
        inputs,
        streamer=streamer,
        max_new_tokens=2048,
        temperature=0.7,
        do_sample=True  # å¼€å¯é‡‡æ ·ï¼Œè®©å›å¤æ›´è‡ªç„¶
    )

    # 4. åœ¨æ–°çº¿ç¨‹ä¸­å¯åŠ¨ç”Ÿæˆ (é˜²æ­¢é˜»å¡ä¸»çº¿ç¨‹æ‰“å°)
    thread = Thread(target=model.generate, kwargs=generation_kwargs)
    thread.start()

    # 5. ä¸»çº¿ç¨‹å®æ—¶æ‰“å°è¾“å‡º
    print(f"ğŸ¤– å›å¤: ", end="", flush=True)
    generated_text = ""
    for new_text in streamer:
        print(new_text, end="", flush=True)
        generated_text += new_text
    print("\n")

    return generated_text


def clear_gpu():
    """æ¸…ç†æ˜¾å­˜"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()
    gc.collect()
    print("ğŸ§¹ æ˜¾å­˜å·²æ¸…ç†\n")


# ================= ğŸš€ ä¸»ç¨‹åº =================
if __name__ == "__main__":
    print(f"â“ é—®é¢˜: {TEST_PROMPT}\n")

    # å…¨å±€ä½¿ç”¨ bfloat16 (é…åˆ 1.5B æ¨¡å‹)
    dtype = torch.bfloat16

    # -------------------------------------------
    # ç¬¬ä¸€æ­¥ï¼šåŸå§‹æ¨¡å‹ (Base)
    # -------------------------------------------
    print("â³ [1/3] æ­£åœ¨åŠ è½½åŸå§‹æ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)

    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_PATH,
        torch_dtype=dtype,
        device_map="auto",
        trust_remote_code=True
    )

    stream_inference(model, tokenizer, TEST_PROMPT, "åŸå§‹æ¨¡å‹ (Base)")

    # é”€æ¯æ¨¡å‹é‡Šæ”¾æ˜¾å­˜
    del model
    clear_gpu()

    # -------------------------------------------
    # ç¬¬äºŒæ­¥ï¼šå…¨é‡å¾®è°ƒæ¨¡å‹ (Checkpoint)
    # -------------------------------------------
    print(f"â³ [2/3] æ­£åœ¨åŠ è½½å…¨é‡å¾®è°ƒ Checkpoint: {CHECKPOINT_PATH_FULL}...")

    # å…¨é‡å¾®è°ƒçš„ Checkpoint å°±æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„å®Œæ•´æ¨¡å‹
    # æˆ‘ä»¬ç›´æ¥ä» checkpoint ç›®å½•åŠ è½½
    model = AutoModelForCausalLM.from_pretrained(
        CHECKPOINT_PATH_FULL,
        torch_dtype=dtype,
        device_map="auto",
        trust_remote_code=True
    )

    stream_inference(model, tokenizer, TEST_PROMPT, "å…¨é‡å¾®è°ƒæ¨¡å‹ (SFT)")



    # é”€æ¯æ¨¡å‹é‡Šæ”¾æ˜¾å­˜
    del model
    clear_gpu()
    # -------------------------------------------
    # ç¬¬ä¸‰æ­¥ï¼šLORAå¾®è°ƒæ¨¡å‹ (Checkpoint)
    # -------------------------------------------
    print(f"â³ [3/3] æ­£åœ¨åŠ è½½LORå¾®è°ƒ Checkpoint: {CHECKPOINT_PATH_LORA}...")
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_PATH,
        # CHECKPOINT_PATH_LORA,
        torch_dtype=dtype,
        device_map="auto",
        trust_remote_code=True
    )
    finetuned_model=PeftModel.from_pretrained(model, CHECKPOINT_PATH_LORA)
    stream_inference(finetuned_model, tokenizer, TEST_PROMPT, "LORAå¾®è°ƒæ¨¡å‹ (SFT)")
    print("âœ… å¯¹æ¯”ç»“æŸ")